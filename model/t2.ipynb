{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 10:12:34.824215: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-04 10:12:34.825930: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 10:12:34.829573: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 10:12:34.838985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733325154.855286 1449922 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733325154.859657 1449922 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 10:12:34.876754: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ahmedabdullahi/anaconda3/envs/tf_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 10:12:42.161890: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"/home/ahmedabdullahi/NLP590/NLPJobsFinder/Data/modelData.csv\")\n",
    "data = data.dropna(subset=[\"city_ascii\", \"iso2\", \"iso3\"])  # Remove missing values\n",
    "\n",
    "# Preprocess the data\n",
    "data[\"labels\"] = data[\"city_ascii\"]\n",
    "unique_labels = data[\"labels\"].unique()\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def encode_texts_and_labels(df):\n",
    "    texts = df[\"iso2\"] + \" \" + df[\"iso3\"] + \" \" + df[\"city_ascii\"]\n",
    "    tokenized = tokenizer.batch_encode_plus(\n",
    "        texts.tolist(),\n",
    "        max_length=64,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    label_ids = [label2id[label] for label in df[\"labels\"]]\n",
    "    return tokenized[\"input_ids\"], tokenized[\"attention_mask\"], tf.convert_to_tensor(label_ids)\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "X_train, attention_train, y_train = encode_texts_and_labels(train_data)\n",
    "X_val, attention_val, y_val = encode_texts_and_labels(val_data)\n",
    "\n",
    "# Adjust target labels for sequence length\n",
    "sequence_length = 64\n",
    "y_train = np.repeat(y_train[:, np.newaxis], sequence_length, axis=1)\n",
    "y_val = np.repeat(y_val[:, np.newaxis], sequence_length, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = TFBertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "18/18 [==============================] - 108s 5s/step - loss: 14.1820 - accuracy: 1.0851e-04 - val_loss: 13.5981 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "18/18 [==============================] - 92s 5s/step - loss: 12.4684 - accuracy: 0.0039 - val_loss: 16.2476 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "18/18 [==============================] - 92s 5s/step - loss: 11.4361 - accuracy: 0.0064 - val_loss: 18.0772 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    {\"input_ids\": X_train, \"attention_mask\": attention_train},\n",
    "    y_train,\n",
    "    validation_data=(\n",
    "        {\"input_ids\": X_val, \"attention_mask\": attention_val},\n",
    "        y_val,\n",
    "    ),\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
