{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"/home/ahmedabdullahi/NLP590/NLPJobsFinder/backend/data/companies.json\", \"r\") as f:\n",
    "    companies = json.load(f)\n",
    "\n",
    "# Define remote keywords and a function to clean up location strings\n",
    "remote_keywords = [\"Remote\", \"Virtual\", \"Work from Home\", \"Telework\", \"Hybrid\"]\n",
    "\n",
    "def clean_location(location):\n",
    "    \"\"\"Cleans location strings for consistency in tagging.\"\"\"\n",
    "    if location:\n",
    "        return location.replace(\",\", \" , \").replace(\"/\", \" / \").strip()\n",
    "    return \"\"\n",
    "\n",
    "# Helper function to annotate entities\n",
    "\n",
    "def annotate_entities(text, location):\n",
    "    \"\"\"\n",
    "    Annotates text with BIO tags for CITY, COUNTRY, and REMOTE.\n",
    "    Handles multi-word location names and excludes punctuation.\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    location_parts = re.findall(r'\\b\\w+\\b', location) if location else []  # Extract only words\n",
    "    location_idx = 0  # Tracks position in the location_parts\n",
    "\n",
    "    words = re.findall(r'\\b\\w+\\b|[,./]', text)  # Split text into words and punctuation\n",
    "    for word in words:\n",
    "        if location_idx < len(location_parts) and word == location_parts[location_idx]:\n",
    "            # Determine whether it's the start (B-) or continuation (I-)\n",
    "            if location_idx == 0 or annotations[-1][1] == \"O\":\n",
    "                # Start of a new entity\n",
    "                if word.lower() in [\"remote\", \"virtual\", \"telework\", \"work\"]:\n",
    "                    annotations.append((word, \"B-REMOTE\"))\n",
    "                elif word.isupper():\n",
    "                    annotations.append((word, \"B-COUNTRY\"))\n",
    "                else:\n",
    "                    annotations.append((word, \"B-CITY\"))\n",
    "            else:\n",
    "                # Continuation of the current entity\n",
    "                if word.lower() in [\"remote\", \"virtual\", \"telework\", \"work\"]:\n",
    "                    annotations.append((word, \"I-REMOTE\"))\n",
    "                elif word.isupper():\n",
    "                    annotations.append((word, \"I-COUNTRY\"))\n",
    "                else:\n",
    "                    annotations.append((word, \"I-CITY\"))\n",
    "            location_idx += 1\n",
    "        else:\n",
    "            # Non-entity words or punctuation\n",
    "            if re.match(r'\\w+', word):  # Only tag non-punctuation as \"O\"\n",
    "                annotations.append((word, \"O\"))\n",
    "            else:\n",
    "                annotations.append((word, \"O\"))  # Punctuation as \"O\"\n",
    "            location_idx = 0  # Reset if there's a mismatch\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Prepare training data\n",
    "training_data = []\n",
    "\n",
    "for company in companies:\n",
    "    location = clean_location(company.get(\"location\", \"\"))\n",
    "    description = company.get(\"description\", \"\")\n",
    "    combined_text = f\"The job is located at {location}. {description}\"\n",
    "    \n",
    "    # Annotate the combined text\n",
    "    annotated_sentence = annotate_entities(combined_text, location)\n",
    "    training_data.append(annotated_sentence)\n",
    "\n",
    "# Convert to BIO format and save\n",
    "bio_data = []\n",
    "for sentence in training_data:\n",
    "    for word, label in sentence:\n",
    "        bio_data.append(f\"{word}\\t{label}\")\n",
    "    bio_data.append(\"\")  # Blank line for sentence separation\n",
    "\n",
    "# Save to a text file\n",
    "with open(\"ner_training_data.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(bio_data))\n",
    "\n",
    "print(\"NER training data has been saved to ner_training_data.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m training_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 42\u001b[0m     tokens, tags \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_ner_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     training_data\u001b[38;5;241m.\u001b[39mappend((tokens, tags))\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Save the training data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m, in \u001b[0;36mgenerate_ner_tags\u001b[0;34m(entry)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_ner_tags\u001b[39m(entry):\n\u001b[0;32m----> 5\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     description \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     tokens, tags \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def generate_ner_tags(entry):\n",
    "    text = entry[\"location\"] if \"location\" in entry else \"\"\n",
    "    description = entry[\"description\"] if \"description\" in entry else \"\"\n",
    "    tokens, tags = [], []\n",
    "\n",
    "    # Tokenize text\n",
    "    words = re.split(r\"(\\W)\", text)  # Split by non-word characters\n",
    "    for word in words:\n",
    "        if word.strip():\n",
    "            tokens.append(word.strip())\n",
    "            tags.append(\"O\")  # Default to 'O'\n",
    "    \n",
    "    # Add tags for locations\n",
    "    for loc in re.findall(r\"[A-Za-z]+(?:[,\\s/][A-Za-z]+)*\", text):\n",
    "        for part in loc.split():\n",
    "            if part in tokens:\n",
    "                tag = \"B-CITY\" if tags[tokens.index(part)] == \"O\" else \"I-CITY\"\n",
    "                tags[tokens.index(part)] = tag\n",
    "    \n",
    "    # Add tags for 'Remote'\n",
    "    if \"Remote\" in text or \"Remote\" in description:\n",
    "        tokens.append(\"Remote\")\n",
    "        tags.append(\"B-REMOTE\")\n",
    "    \n",
    "    # Tokenize and append description\n",
    "    for word in re.split(r\"(\\W)\", description):\n",
    "        if word.strip():\n",
    "            tokens.append(word.strip())\n",
    "            tags.append(\"O\")\n",
    "    \n",
    "    return tokens, tags\n",
    "\n",
    "with open(\"/home/ahmedabdullahi/NLP590/NLPJobsFinder/Data/combined_location_description.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Generate training data\n",
    "training_data = []\n",
    "for entry in data:\n",
    "    tokens, tags = generate_ner_tags(entry)\n",
    "    training_data.append((tokens, tags))\n",
    "\n",
    "# Save the training data\n",
    "with open(\"ner_training_data1.txt\", \"w\") as f:\n",
    "    for tokens, tags in training_data:\n",
    "        for token, tag in zip(tokens, tags):\n",
    "            f.write(f\"{token}\\t{tag}\\n\")\n",
    "        f.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
